{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "14d6c2848630e49710635a48091d83ec66863a9274f829339ae06d8647561ca8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Modern Monte Carlo Tree Search\n",
    "\n",
    "How can we efficiently search without relying on expert knowledge?\n",
    "\n",
    "- **exploration**: learn values of actions we are uncertain about\n",
    "- **exploitation**: focus search on promising parts of tree\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Multi-Armed bandit\n",
    "\n",
    "- MDP with no state\n",
    "- we want to minimize the total regret (how much reward you lost by not selecting optimal action up to time T)\n",
    "\n",
    "- **information state search**: exploration provides information which can increase expected reward in future \n",
    "- optimal solutions can be found by solving infinte-state MDP over info states\n",
    "- need heuristics to make things tractable\n",
    "\n",
    "bandit strategies scale with how we handle uncertainty:\n",
    "\n",
    "- no exploration, just greedily pull best values we have seen in past\n",
    "- random exploration\n",
    "- explore with preference towards uncertaint\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### $\\epsilon$-Greedy Algorithm\n",
    "\n",
    "action value is just average of rewards we have seen over time\n",
    "\n",
    "e-Greedy just selects the non-max at random sometimes\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### UCB Algorithm\n",
    "\n",
    "e-Greedy not awesome because a lot of the time we end up exploring bad actions that we sort of know will not yield anything great\n",
    "\n",
    "The idea is to agument the value function with an upper bound on the reward value so that the true reward value is lower: $Q(a) \\leq \\hat{Q}_t(a) + \\hat{U}_t(a)$.\n",
    "\n",
    "Hoeffding's inequality can be used if we do not want to assign any prior to the distribution of rewards, works for any bounded distribution. \n",
    "\n",
    "$$P[\\frac{1}{n} \\sum_{i =1}^{n}(Z_i - E[Z_i]) \\geq \\delta\\ ] \\leq e^{- \\frac{2n\\delta^2}{(b - a)^2}}$$\n",
    "\n",
    "So we view this average of the random variable $Z_i$ as the averaged reward of a particular action, $\\hat{Q_t}(a) = \\frac{1}{n} \\sum_{t=0}^n R_i$. The inequality then gets written as (with $t = N_t(a)$, the number of times this action has been visited): \n",
    "\n",
    "$$P[Q_t - \\hat{Q_t} \\geq \\delta ] \\leq e^{-\\frac{2N_t(a)\\delta^2}{(b-a)^2}}$$\n",
    "\n",
    "We see that then our upper bound on the true reward can be given by $\\delta = \\hat{U_t}(a)$. And if we make the simplifying assumption that the reward is bounded to be on the interval $(0, 1)$. We can then for any value of the probability of deviation, $p$, we can solve for what we expect the upper bound on that deviation to be:\n",
    "\n",
    "$$\n",
    "p \\approx e^{-2tU_t(a)^2} \\\\\n",
    "U_t(a) = \\sqrt{\\frac{- \\log p}{2 N_t (a)}}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### UCB 1\n",
    "Most of the time when we see the UCB algorithm in practice (like monte carlo tree search), we see it in the guise of \n",
    "### "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}